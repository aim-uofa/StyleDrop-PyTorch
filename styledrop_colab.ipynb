{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neild0/StyleDrop-PyTorch-Interactive/blob/main/styledrop_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "546a1e28-bbe1-4cac-832c-090d58aa4b5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "546a1e28-bbe1-4cac-832c-090d58aa4b5f",
        "outputId": "07f1a0bc-6970-4402-9a8a-0fc23a1b19fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'StyleDrop-PyTorch-Interactive'...\n",
            "remote: Enumerating objects: 373, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 373 (delta 43), reused 37 (delta 14), pack-reused 289\u001b[K\n",
            "Receiving objects: 100% (373/373), 44.01 MiB | 20.08 MiB/s, done.\n",
            "Resolving deltas: 100% (97/97), done.\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting accelerate==0.12.0\n",
            "  Downloading accelerate-0.12.0-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.0/144.0 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Collecting ml_collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy==6.1.1\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.23.1\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webdataset==0.2.5\n",
            "  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio\n",
            "  Downloading gradio-3.36.1-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.12.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.12.0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.12.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.12.0) (6.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.12.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.1.1) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0 (from transformers==4.23.1)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.23.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (4.65.0)\n",
            "Collecting braceexpand (from webdataset==0.2.5)\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections) (0.6.0.post1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.27.1-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.7/211.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Collecting aiofiles (from gradio)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.7 (from gradio)\n",
            "  Downloading gradio_client-0.2.7-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from gradio)\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.9)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyre-extensions==0.0.29 (from xformers)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.12.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.12.0) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.12.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.12.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.12.0) (16.0.6)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (2023.6.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (3.4)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.1.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (1.7.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->accelerate==0.12.0) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, ml_collections, ffmpy, pathtools\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=bf97436a11c5b10528d69ee11dc45a88407c5b84c054c2faf5ef9e6482508a98\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml_collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=57cadb30a2d9132d6e3bae2a5693c789f1e718c011cf4e299896ee95463c13d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=6e0230d6f3a8c65be08eca0444bd8f6e42cd27f5a4e175c765019a549fbbb70b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=5f79dded9f8b3fb5f01c91861cd6b75d993dfd6270cad2fbdd038fcaf31d8d9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built antlr4-python3-runtime ml_collections ffmpy pathtools\n",
            "Installing collected packages: tokenizers, pydub, pathtools, ffmpy, braceexpand, antlr4-python3-runtime, websockets, webdataset, uc-micro-py, smmap, setproctitle, sentry-sdk, semantic-version, python-multipart, orjson, omegaconf, mypy-extensions, ml_collections, markdown-it-py, loguru, h11, ftfy, einops, docker-pycreds, aiofiles, uvicorn, typing-inspect, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, gitdb, transformers, pyre-extensions, httpx, GitPython, fastapi, wandb, gradio-client, gradio, xformers, accelerate\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "Successfully installed GitPython-3.1.31 accelerate-0.12.0 aiofiles-23.1.0 antlr4-python3-runtime-4.9.3 braceexpand-0.1.7 docker-pycreds-0.4.0 einops-0.6.1 fastapi-0.100.0 ffmpy-0.3.0 ftfy-6.1.1 gitdb-4.0.10 gradio-3.36.1 gradio-client-0.2.7 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 linkify-it-py-2.0.2 loguru-0.7.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 ml_collections-0.1.1 mypy-extensions-1.0.0 omegaconf-2.3.0 orjson-3.9.2 pathtools-0.1.2 pydub-0.25.1 pyre-extensions-0.0.29 python-multipart-0.0.6 semantic-version-2.10.0 sentry-sdk-1.27.1 setproctitle-1.3.2 smmap-5.0.0 starlette-0.27.0 tokenizers-0.13.3 transformers-4.23.1 typing-inspect-0.9.0 uc-micro-py-1.0.2 uvicorn-0.22.0 wandb-0.15.5 webdataset-0.2.5 websockets-11.0.3 xformers-0.0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/neild0/StyleDrop-PyTorch-Interactive\n",
        "!mv StyleDrop-PyTorch-Interactive StyleDrop-PyTorch\n",
        "\n",
        "!pip install omegaconf gdown accelerate==0.12.0 absl-py ml_collections einops wandb ftfy==6.1.1 transformers==4.23.1 loguru webdataset==0.2.5 gradio xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/nzl-thu/MUSE\n",
        "!mv MUSE/assets/* StyleDrop-PyTorch/assets/\n",
        "!rm -rf MUSE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bpgLXkHcg8v",
        "outputId": "867e16b5-fbdc-40f9-fcc9-d7b34b775793"
      },
      "id": "0bpgLXkHcg8v",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into 'MUSE'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Total 25 (delta 0), reused 0 (delta 0), pack-reused 25\u001b[K\n",
            "Unpacking objects: 100% (25/25), 3.01 KiB | 770.00 KiB/s, done.\n",
            "Filtering content: 100% (12/12), 9.08 GiB | 149.66 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tassets/ckpts/cc3m-285000.ckpt/optimizer.pth\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id '13S_unB87n6KKuuMdyMnyExW0G1kplTbP' --output StyleDrop-PyTorch/assets/vqgan_jax_strongaug.ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJZbaAOVY3_3",
        "outputId": "29b5d7ee-ec97-4697-d992-32fc84e90522"
      },
      "id": "IJZbaAOVY3_3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13S_unB87n6KKuuMdyMnyExW0G1kplTbP\n",
            "To: /content/StyleDrop-PyTorch/assets/vqgan_jax_strongaug.ckpt\n",
            "100% 218M/218M [00:02<00:00, 90.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python StyleDrop-PyTorch/extract_empty_feature.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVInHq6dbAa0",
        "outputId": "b7250bd3-e86f-4dd0-d803-cc3bff8f85a5"
      },
      "id": "sVInHq6dbAa0",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-07-08 08:32:42.610\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_clip.transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m314\u001b[0m - \u001b[34m\u001b[1mxattn in transformer of CLIP is True\u001b[0m\n",
            "\u001b[32m2023-07-08 08:32:57.045\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_clip.transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m314\u001b[0m - \u001b[34m\u001b[1mxattn in transformer of CLIP is True\u001b[0m\n",
            "Downloading (…)ip_pytorch_model.bin: 100% 10.2G/10.2G [00:37<00:00, 268MB/s]\n",
            "torch.Size([1, 77, 1280])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import subprocess\n",
        "\n",
        "# if 'EVAL_CKPT' in os.environ:\n",
        "#     del os.environ['EVAL_CKPT']\n",
        "\n",
        "# if 'ADAPTER' in os.environ:\n",
        "#     del os.environ['ADAPTER']\n",
        "\n",
        "# os.environ['OUTPUT_DIR'] = \"training_data\"\n",
        "\n",
        "# subprocess.run(['accelerate', 'launch', '--num_processes', '8', '--mixed_precision', 'fp16', 'StyleDrop-PyTorch/train_t2i_custom_v2.py', '--config=StyleDrop-PyTorch/configs/custom.py'])"
      ],
      "metadata": {
        "id": "epmOxNBY4BbL"
      },
      "id": "epmOxNBY4BbL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd StyleDrop-PyTorch && accelerate launch --mixed_precision fp16 train_t2i_colab_v2.py --config=configs/custom.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efU3-4c7E6TN",
        "outputId": "b19a86f2-ed44-4c15-c9c4-499da5db4a6b"
      },
      "id": "efU3-4c7E6TN",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-08 08:34:04.602120: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-08 08:34:04.657399: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-08 08:34:05.516847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--num_cpu_threads_per_process` was set to `6` to improve out-of-box performance\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2023-07-08 08:34:10.351161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "Moving 0 files to the new cache system\n",
            "0it [00:00, ?it/s]\n",
            "spawned\n",
            "\u001b[32m2023-07-08 08:34:13.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mProcess 0 using device: cuda\u001b[0m\n",
            "I0708 08:34:13.686498 140426273797952 factory.py:158] Loaded ViT-bigG-14 model config.\n",
            "\u001b[32m2023-07-08 08:34:13.699\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_clip.transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m314\u001b[0m - \u001b[34m\u001b[1mxattn in transformer of CLIP is True\u001b[0m\n",
            "\u001b[32m2023-07-08 08:34:28.386\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_clip.transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m314\u001b[0m - \u001b[34m\u001b[1mxattn in transformer of CLIP is True\u001b[0m\n",
            "I0708 08:34:37.477463 140426273797952 factory.py:206] Loading pretrained ViT-bigG-14 weights (laion2b_s39b_b160k).\n",
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "Strict load\n",
            "Restored from assets/vqgan_jax_strongaug.ckpt\n",
            "\u001b[32m2023-07-08 08:34:50.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1madapter_path: null\n",
            "autoencoder:\n",
            "  config_file: vq-f16-jax.yaml\n",
            "ckpt_root: /root/exp/default/0708_083413/ckpts_II\n",
            "config_name: custom\n",
            "data_path: data/one_style.json\n",
            "disable_val: false\n",
            "lr_scheduler:\n",
            "  name: customized\n",
            "  warmup_steps: -1\n",
            "mixed_precision: fp16\n",
            "muse:\n",
            "  gen_temp: 4.5\n",
            "  ignore_ind: -1\n",
            "  smoothing: 0.1\n",
            "nnet:\n",
            "  clip_dim: 1280\n",
            "  codebook_size: 1024\n",
            "  d_prj: 32\n",
            "  depth: 28\n",
            "  embed_dim: 1152\n",
            "  img_size: 16\n",
            "  in_chans: 4\n",
            "  is_shared: false\n",
            "  mlp_ratio: 4\n",
            "  name: uvit_t2i_vq\n",
            "  num_clip_token: 77\n",
            "  num_heads: 16\n",
            "  qkv_bias: false\n",
            "  skip: true\n",
            "  use_checkpoint: false\n",
            "optimizer:\n",
            "  betas: !!python/tuple\n",
            "  - 0.99\n",
            "  - 0.99\n",
            "  lr: 0.0003\n",
            "  name: adamw\n",
            "  weight_decay: 0.03\n",
            "resume_root: assets/ckpts/cc3m-285000.ckpt\n",
            "sample:\n",
            "  cfg: true\n",
            "  lambdaA: 2.0\n",
            "  lambdaB: 5.0\n",
            "  linear_inc_scale: true\n",
            "  mini_batch_size: 8\n",
            "  n_samples: 50\n",
            "  path: ''\n",
            "  sample_steps: 36\n",
            "  scale: 10.0\n",
            "sample_dir: /root/exp/default/0708_083413/samples_II\n",
            "sample_interval: true\n",
            "seed: 1234\n",
            "train:\n",
            "  batch_size: 8\n",
            "  eval_interval: 100\n",
            "  fid_interval: 20000\n",
            "  log_interval: 20\n",
            "  n_steps: 1000\n",
            "  num_workers: 8\n",
            "  resampled: false\n",
            "  save_interval: 100\n",
            "workdir: !!python/object/apply:pathlib.PosixPath\n",
            "- /\n",
            "- root\n",
            "- exp\n",
            "- default\n",
            "- 0708_083413\n",
            "z_shape: !!python/tuple\n",
            "- 8\n",
            "- 16\n",
            "- 16\n",
            "\u001b[0m\n",
            "\u001b[32m2023-07-08 08:34:50.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mRun on 1 devices\u001b[0m\n",
            "\u001b[32m2023-07-08 08:34:50.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mworld size is 1\u001b[0m\n",
            "---------------------------------------------------\n",
            "train dataset length:  1\n",
            "train dataset length:  1\n",
            "A house in watercolor painting style\n",
            "data/image_01_02.jpg\n",
            "---------------------------------------------------\n",
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "Strict load\n",
            "Restored from assets/vqgan_jax_strongaug.ckpt\n",
            "xformers available, will use xformers attention\n",
            "\u001b[32m2023-07-08 08:34:50.670\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mlibs.uvit_t2i_vq\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m200\u001b[0m - \u001b[34m\u001b[1mcodebook size in nnet: 1024\u001b[0m\n",
            "num vis tokens: 256\n",
            "\u001b[32m2023-07-08 08:34:57.250\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mlibs.uvit_t2i_vq\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m200\u001b[0m - \u001b[34m\u001b[1mcodebook size in nnet: 1024\u001b[0m\n",
            "num vis tokens: 256\n",
            "\u001b[32m2023-07-08 08:35:03.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36minitialize_train_state\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mnnet has 505693313 parameters\u001b[0m\n",
            "\u001b[32m2023-07-08 08:35:04.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mresume\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mresume from assets/ckpts/cc3m-285000.ckpt\u001b[0m\n",
            "\u001b[32m2023-07-08 08:35:04.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mload from assets/ckpts/cc3m-285000.ckpt\u001b[0m\n",
            "\u001b[32m2023-07-08 08:35:06.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m264\u001b[0m - \u001b[1mStart fitting, step=285000, mixed_precision=fp16\u001b[0m\n",
            "epoch: 100% 3/3 [00:05<00:00,  1.80s/it]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.81it/s]\u001b[32m2023-07-08 08:35:18.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285020 data_time: 0.2427 (0.4565) loss: 6.0395 (6.0374) loss_scale: 65536.0000 (65536.0000) grad_norm: 0.1877 (0.2001)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:35:25.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285040 data_time: 0.2436 (0.3505) loss: 5.6383 (5.8455) loss_scale: 65536.0000 (65536.0000) grad_norm: 0.3888 (0.3133)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.79it/s]\u001b[32m2023-07-08 08:35:32.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285060 data_time: 0.2427 (0.3146) loss: 5.3329 (5.6613) loss_scale: 65536.0000 (65536.0000) grad_norm: 0.7824 (0.4684)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:35:39.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285080 data_time: 0.2434 (0.2970) loss: 4.9544 (5.4878) loss_scale: 65536.0000 (65536.0000) grad_norm: 0.8854 (0.5976)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:35:47.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mSave checkpoint 285100...\u001b[0m\n",
            "\u001b[32m2023-07-08 08:35:47.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1meval_step: n_samples=50, sample_steps=36mini_batch_size=8\u001b[0m\n",
            "\u001b[32m2023-07-08 08:35:47.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mPath for Eval images: /root/exp/default/0708_083413/eval_samples/285100_0708_083547\u001b[0m\n",
            "\n",
            "sample2dir:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "sample2dir:  14% 1/7 [00:14<01:26, 14.46s/it]\u001b[A\n",
            "sample2dir:  29% 2/7 [00:28<01:12, 14.44s/it]\u001b[A\n",
            "sample2dir:  43% 3/7 [00:43<00:57, 14.42s/it]\u001b[A\n",
            "sample2dir:  57% 4/7 [00:57<00:43, 14.39s/it]\u001b[A\n",
            "sample2dir:  71% 5/7 [01:11<00:28, 14.37s/it]\u001b[A\n",
            "sample2dir:  86% 6/7 [01:24<00:13, 13.85s/it]\u001b[A\n",
            "sample2dir: 100% 7/7 [01:39<00:00, 14.15s/it]\n",
            "\u001b[32m2023-07-08 08:37:26.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285100 data_time: 0.2431 (0.2863) loss: 4.5973 (5.3167) loss_scale: 65536.0000 (65536.0000) grad_norm: 1.3441 (0.7852)\u001b[0m\n",
            "\u001b[32m2023-07-08 08:37:26.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSave a grid of images...\u001b[0m\n",
            "Eval prompt: A chihuahua in watercolor painting style\n",
            "Shape of contexts :[torch.Size([10, 77, 1280])]\n",
            "epoch: 100% 3/3 [01:56<00:00, 38.85s/it]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:37:49.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285120 data_time: 0.2426 (0.2791) loss: 4.3380 (5.1547) loss_scale: 65536.0000 (65536.0000) grad_norm: 1.7205 (0.9833)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.79it/s]\u001b[32m2023-07-08 08:37:56.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285140 data_time: 0.2429 (0.2741) loss: 3.9065 (4.9792) loss_scale: 65536.0000 (65536.0000) grad_norm: 1.8917 (1.1617)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:38:04.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285160 data_time: 0.2425 (0.2702) loss: 3.6024 (4.8078) loss_scale: 65536.0000 (65536.0000) grad_norm: 2.3854 (1.3816)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.71it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.69it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.78it/s]\u001b[32m2023-07-08 08:38:11.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285180 data_time: 0.2433 (0.2674) loss: 3.1070 (4.6312) loss_scale: 65536.0000 (65536.0000) grad_norm: 2.2963 (1.5696)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.79it/s]\u001b[32m2023-07-08 08:38:18.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mSave checkpoint 285200...\u001b[0m\n",
            "\u001b[32m2023-07-08 08:38:18.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1meval_step: n_samples=50, sample_steps=36mini_batch_size=8\u001b[0m\n",
            "\u001b[32m2023-07-08 08:38:18.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mPath for Eval images: /root/exp/default/0708_083413/eval_samples/285200_0708_083818\u001b[0m\n",
            "\n",
            "sample2dir:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "sample2dir:  14% 1/7 [00:14<01:26, 14.43s/it]\u001b[A\n",
            "sample2dir:  29% 2/7 [00:28<01:12, 14.42s/it]\u001b[A\n",
            "sample2dir:  43% 3/7 [00:43<00:57, 14.40s/it]\u001b[A\n",
            "sample2dir:  57% 4/7 [00:57<00:43, 14.39s/it]\u001b[A\n",
            "sample2dir:  71% 5/7 [01:10<00:27, 13.91s/it]\u001b[A\n",
            "sample2dir:  86% 6/7 [01:25<00:14, 14.06s/it]\u001b[A\n",
            "sample2dir: 100% 7/7 [01:39<00:00, 14.18s/it]\n",
            "\u001b[32m2023-07-08 08:39:57.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285200 data_time: 0.2429 (0.2650) loss: 2.6884 (4.4578) loss_scale: 65536.0000 (65536.0000) grad_norm: 2.1294 (1.8037)\u001b[0m\n",
            "\u001b[32m2023-07-08 08:39:57.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSave a grid of images...\u001b[0m\n",
            "Eval prompt: A chihuahua in watercolor painting style\n",
            "Shape of contexts :[torch.Size([10, 77, 1280])]\n",
            "epoch: 100% 3/3 [01:56<00:00, 38.91s/it]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:40:21.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285220 data_time: 0.2426 (0.2630) loss: 2.6247 (4.2999) loss_scale: 65536.0000 (65536.0000) grad_norm: 3.1107 (1.9955)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.72it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:40:28.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285240 data_time: 0.2436 (0.2615) loss: 2.3096 (4.1381) loss_scale: 65536.0000 (65536.0000) grad_norm: 3.1702 (2.1328)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.79it/s]\u001b[32m2023-07-08 08:40:35.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285260 data_time: 0.2428 (0.2601) loss: 1.8813 (3.9820) loss_scale: 65536.0000 (65536.0000) grad_norm: 2.6088 (2.2408)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:40:42.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285280 data_time: 0.2438 (0.2589) loss: 2.1029 (3.8508) loss_scale: 65536.0000 (65536.0000) grad_norm: 4.3948 (2.5058)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.78it/s]\u001b[32m2023-07-08 08:40:50.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mSave checkpoint 285300...\u001b[0m\n",
            "\u001b[32m2023-07-08 08:40:50.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1meval_step: n_samples=50, sample_steps=36mini_batch_size=8\u001b[0m\n",
            "\u001b[32m2023-07-08 08:40:50.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mPath for Eval images: /root/exp/default/0708_083413/eval_samples/285300_0708_084050\u001b[0m\n",
            "\n",
            "sample2dir:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "sample2dir:  14% 1/7 [00:14<01:26, 14.46s/it]\u001b[A\n",
            "sample2dir:  29% 2/7 [00:28<01:12, 14.47s/it]\u001b[A\n",
            "sample2dir:  43% 3/7 [00:43<00:57, 14.43s/it]\u001b[A\n",
            "sample2dir:  57% 4/7 [00:56<00:41, 13.80s/it]\u001b[A\n",
            "sample2dir:  71% 5/7 [01:10<00:28, 14.06s/it]\u001b[A\n",
            "sample2dir:  86% 6/7 [01:25<00:14, 14.19s/it]\u001b[A\n",
            "sample2dir: 100% 7/7 [01:39<00:00, 14.20s/it]\n",
            "\u001b[32m2023-07-08 08:42:29.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285300 data_time: 0.2434 (0.2579) loss: 1.8801 (3.7310) loss_scale: 65536.0000 (64989.8667) grad_norm: 4.6782 (inf)\u001b[0m\n",
            "\u001b[32m2023-07-08 08:42:29.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSave a grid of images...\u001b[0m\n",
            "Eval prompt: A chihuahua in watercolor painting style\n",
            "Shape of contexts :[torch.Size([10, 77, 1280])]\n",
            "epoch: 100% 3/3 [01:56<00:00, 38.94s/it]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:42:53.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285320 data_time: 0.2429 (0.2570) loss: 1.5457 (3.6079) loss_scale: 32768.0000 (62976.0000) grad_norm: 2.1761 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:43:00.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285340 data_time: 0.2433 (0.2563) loss: 1.4611 (3.4996) loss_scale: 32768.0000 (61199.0588) grad_norm: 1.6780 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.79it/s]\u001b[32m2023-07-08 08:43:07.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285360 data_time: 0.2422 (0.2555) loss: 1.4443 (3.3992) loss_scale: 32768.0000 (59619.5556) grad_norm: 2.9502 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:43:14.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285380 data_time: 0.2436 (0.2549) loss: 1.4066 (3.3056) loss_scale: 32768.0000 (58206.3158) grad_norm: 1.8023 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:43:21.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mSave checkpoint 285400...\u001b[0m\n",
            "\u001b[32m2023-07-08 08:43:21.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1meval_step: n_samples=50, sample_steps=36mini_batch_size=8\u001b[0m\n",
            "\u001b[32m2023-07-08 08:43:21.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mPath for Eval images: /root/exp/default/0708_083413/eval_samples/285400_0708_084321\u001b[0m\n",
            "\n",
            "sample2dir:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "sample2dir:  14% 1/7 [00:14<01:26, 14.34s/it]\u001b[A\n",
            "sample2dir:  29% 2/7 [00:28<01:11, 14.35s/it]\u001b[A\n",
            "sample2dir:  43% 3/7 [00:41<00:54, 13.64s/it]\u001b[A\n",
            "sample2dir:  57% 4/7 [00:55<00:41, 13.95s/it]\u001b[A\n",
            "sample2dir:  71% 5/7 [01:10<00:28, 14.11s/it]\u001b[A\n",
            "sample2dir:  86% 6/7 [01:24<00:14, 14.22s/it]\u001b[A\n",
            "sample2dir: 100% 7/7 [01:39<00:00, 14.15s/it]\n",
            "\u001b[32m2023-07-08 08:45:00.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285400 data_time: 0.2424 (0.2543) loss: 1.6673 (3.2286) loss_scale: 32768.0000 (56934.4000) grad_norm: 3.1544 (inf)\u001b[0m\n",
            "\u001b[32m2023-07-08 08:45:00.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSave a grid of images...\u001b[0m\n",
            "Eval prompt: A chihuahua in watercolor painting style\n",
            "Shape of contexts :[torch.Size([10, 77, 1280])]\n",
            "epoch: 100% 3/3 [01:56<00:00, 38.83s/it]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:45:24.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285420 data_time: 0.2425 (0.2538) loss: 1.5596 (3.1520) loss_scale: 32768.0000 (55783.6190) grad_norm: 1.6966 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:45:31.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285440 data_time: 0.2434 (0.2533) loss: 1.2794 (3.0720) loss_scale: 32768.0000 (54737.4545) grad_norm: 1.0704 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:45:38.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285460 data_time: 0.2426 (0.2529) loss: 1.2638 (3.0048) loss_scale: 32768.0000 (53782.2609) grad_norm: 1.5728 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:45:45.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285480 data_time: 0.2438 (0.2525) loss: 1.2365 (2.9446) loss_scale: 32768.0000 (52906.6667) grad_norm: 0.8990 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.78it/s]\u001b[32m2023-07-08 08:45:53.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mSave checkpoint 285500...\u001b[0m\n",
            "\u001b[32m2023-07-08 08:45:53.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1meval_step: n_samples=50, sample_steps=36mini_batch_size=8\u001b[0m\n",
            "\u001b[32m2023-07-08 08:45:53.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mPath for Eval images: /root/exp/default/0708_083413/eval_samples/285500_0708_084553\u001b[0m\n",
            "\n",
            "sample2dir:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "sample2dir:  14% 1/7 [00:14<01:26, 14.46s/it]\u001b[A\n",
            "sample2dir:  29% 2/7 [00:27<01:07, 13.54s/it]\u001b[A\n",
            "sample2dir:  43% 3/7 [00:41<00:55, 13.97s/it]\u001b[A\n",
            "sample2dir:  57% 4/7 [00:56<00:42, 14.15s/it]\u001b[A\n",
            "sample2dir:  71% 5/7 [01:10<00:28, 14.25s/it]\u001b[A\n",
            "sample2dir:  86% 6/7 [01:25<00:14, 14.33s/it]\u001b[A\n",
            "sample2dir: 100% 7/7 [01:39<00:00, 14.20s/it]\n",
            "\u001b[32m2023-07-08 08:47:32.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285500 data_time: 0.2428 (0.2522) loss: 1.3122 (2.8917) loss_scale: 32768.0000 (52101.1200) grad_norm: 1.9016 (inf)\u001b[0m\n",
            "\u001b[32m2023-07-08 08:47:32.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSave a grid of images...\u001b[0m\n",
            "Eval prompt: A chihuahua in watercolor painting style\n",
            "Shape of contexts :[torch.Size([10, 77, 1280])]\n",
            "epoch: 100% 3/3 [01:56<00:00, 38.97s/it]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:47:56.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285520 data_time: 0.2433 (0.2518) loss: 1.3035 (2.8414) loss_scale: 32768.0000 (51357.5385) grad_norm: 1.5092 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.79it/s]\u001b[32m2023-07-08 08:48:03.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285540 data_time: 0.2459 (0.2516) loss: 1.3265 (2.7900) loss_scale: 32768.0000 (50669.0370) grad_norm: 2.0141 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.76it/s]\u001b[32m2023-07-08 08:48:10.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285560 data_time: 0.2427 (0.2513) loss: 1.2239 (2.7396) loss_scale: 32768.0000 (50029.7143) grad_norm: 1.3691 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.73it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:48:17.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285580 data_time: 0.2439 (0.2511) loss: 1.1876 (2.6931) loss_scale: 32768.0000 (49434.4828) grad_norm: 0.7182 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.72it/s]\u001b[32m2023-07-08 08:48:24.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mSave checkpoint 285600...\u001b[0m\n",
            "\u001b[32m2023-07-08 08:48:25.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1meval_step: n_samples=50, sample_steps=36mini_batch_size=8\u001b[0m\n",
            "\u001b[32m2023-07-08 08:48:25.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mPath for Eval images: /root/exp/default/0708_083413/eval_samples/285600_0708_084825\u001b[0m\n",
            "\n",
            "sample2dir:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "sample2dir:  14% 1/7 [00:12<01:17, 12.91s/it]\u001b[A\n",
            "sample2dir:  29% 2/7 [00:27<01:09, 13.81s/it]\u001b[A\n",
            "sample2dir:  43% 3/7 [00:41<00:56, 14.11s/it]\u001b[A\n",
            "sample2dir:  57% 4/7 [00:56<00:42, 14.25s/it]\u001b[A\n",
            "sample2dir:  71% 5/7 [01:10<00:28, 14.29s/it]\u001b[A\n",
            "sample2dir:  86% 6/7 [01:25<00:14, 14.34s/it]\u001b[A\n",
            "sample2dir: 100% 7/7 [01:37<00:00, 13.97s/it]\n",
            "\u001b[32m2023-07-08 08:50:02.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285600 data_time: 0.2435 (0.2509) loss: 1.5845 (2.6563) loss_scale: 32768.0000 (48878.9333) grad_norm: 2.8926 (inf)\u001b[0m\n",
            "\u001b[32m2023-07-08 08:50:02.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSave a grid of images...\u001b[0m\n",
            "Eval prompt: A chihuahua in watercolor painting style\n",
            "Shape of contexts :[torch.Size([10, 77, 1280])]\n",
            "epoch: 100% 3/3 [01:55<00:00, 38.41s/it]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.79it/s]\u001b[32m2023-07-08 08:50:26.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285620 data_time: 0.2423 (0.2506) loss: 1.4649 (2.6228) loss_scale: 32768.0000 (48359.2258) grad_norm: 3.0660 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:50:33.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285640 data_time: 0.2459 (0.2504) loss: 1.1963 (2.5852) loss_scale: 32768.0000 (47872.0000) grad_norm: 2.3140 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:50:40.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285660 data_time: 0.2428 (0.2502) loss: 1.1619 (2.5441) loss_scale: 32768.0000 (47414.3030) grad_norm: 0.6048 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.82it/s]\u001b[32m2023-07-08 08:50:47.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285680 data_time: 0.2464 (0.2501) loss: 1.2083 (2.5093) loss_scale: 32768.0000 (46983.5294) grad_norm: 1.4383 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:50:55.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mSave checkpoint 285700...\u001b[0m\n",
            "\u001b[32m2023-07-08 08:50:55.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1meval_step: n_samples=50, sample_steps=36mini_batch_size=8\u001b[0m\n",
            "\u001b[32m2023-07-08 08:50:55.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mPath for Eval images: /root/exp/default/0708_083413/eval_samples/285700_0708_085055\u001b[0m\n",
            "\n",
            "sample2dir:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "sample2dir:  14% 1/7 [00:14<01:26, 14.43s/it]\u001b[A\n",
            "sample2dir:  29% 2/7 [00:28<01:11, 14.39s/it]\u001b[A\n",
            "sample2dir:  43% 3/7 [00:43<00:57, 14.42s/it]\u001b[A\n",
            "sample2dir:  57% 4/7 [00:57<00:43, 14.41s/it]\u001b[A\n",
            "sample2dir:  71% 5/7 [01:11<00:28, 14.39s/it]\u001b[A\n",
            "sample2dir:  86% 6/7 [01:24<00:13, 13.85s/it]\u001b[A\n",
            "sample2dir: 100% 7/7 [01:39<00:00, 14.17s/it]\n",
            "\u001b[32m2023-07-08 08:52:34.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285700 data_time: 0.2422 (0.2499) loss: 1.1494 (2.4750) loss_scale: 32768.0000 (46577.3714) grad_norm: 0.6607 (inf)\u001b[0m\n",
            "\u001b[32m2023-07-08 08:52:34.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSave a grid of images...\u001b[0m\n",
            "Eval prompt: A chihuahua in watercolor painting style\n",
            "Shape of contexts :[torch.Size([10, 77, 1280])]\n",
            "epoch: 100% 3/3 [01:56<00:00, 38.88s/it]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.79it/s]\u001b[32m2023-07-08 08:52:57.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285720 data_time: 0.2433 (0.2497) loss: 1.1324 (2.4413) loss_scale: 32768.0000 (46193.7778) grad_norm: 0.3615 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.72it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.81it/s]\u001b[32m2023-07-08 08:53:05.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285740 data_time: 0.2437 (0.2496) loss: 1.1300 (2.4105) loss_scale: 32768.0000 (45830.9189) grad_norm: 0.3747 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:53:12.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285760 data_time: 0.2429 (0.2494) loss: 1.1223 (2.3802) loss_scale: 32768.0000 (45487.1579) grad_norm: 0.3220 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.81it/s]\u001b[32m2023-07-08 08:53:19.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285780 data_time: 0.2467 (0.2493) loss: 1.1208 (2.3499) loss_scale: 32768.0000 (45161.0256) grad_norm: 0.3488 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.78it/s]\u001b[32m2023-07-08 08:53:26.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mSave checkpoint 285800...\u001b[0m\n",
            "\u001b[32m2023-07-08 08:53:26.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1meval_step: n_samples=50, sample_steps=36mini_batch_size=8\u001b[0m\n",
            "\u001b[32m2023-07-08 08:53:26.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mPath for Eval images: /root/exp/default/0708_083413/eval_samples/285800_0708_085326\u001b[0m\n",
            "\n",
            "sample2dir:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "sample2dir:  14% 1/7 [00:14<01:26, 14.38s/it]\u001b[A\n",
            "sample2dir:  29% 2/7 [00:28<01:11, 14.40s/it]\u001b[A\n",
            "sample2dir:  43% 3/7 [00:43<00:57, 14.39s/it]\u001b[A\n",
            "sample2dir:  57% 4/7 [00:57<00:43, 14.35s/it]\u001b[A\n",
            "sample2dir:  71% 5/7 [01:10<00:27, 13.83s/it]\u001b[A\n",
            "sample2dir:  86% 6/7 [01:24<00:14, 14.03s/it]\u001b[A\n",
            "sample2dir: 100% 7/7 [01:39<00:00, 14.16s/it]\n",
            "\u001b[32m2023-07-08 08:55:05.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285800 data_time: 0.2427 (0.2492) loss: 1.1357 (2.3241) loss_scale: 32768.0000 (44851.2000) grad_norm: 0.9469 (inf)\u001b[0m\n",
            "\u001b[32m2023-07-08 08:55:05.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSave a grid of images...\u001b[0m\n",
            "Eval prompt: A chihuahua in watercolor painting style\n",
            "Shape of contexts :[torch.Size([10, 77, 1280])]\n",
            "epoch: 100% 3/3 [01:56<00:00, 38.87s/it]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:55:29.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285820 data_time: 0.2429 (0.2490) loss: 1.1256 (2.3007) loss_scale: 32768.0000 (44556.4878) grad_norm: 0.5969 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.72it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:55:36.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285840 data_time: 0.2432 (0.2489) loss: 1.1258 (2.2755) loss_scale: 32768.0000 (44275.8095) grad_norm: 0.4875 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.79it/s]\u001b[32m2023-07-08 08:55:43.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285860 data_time: 0.2428 (0.2488) loss: 1.1459 (2.2535) loss_scale: 32768.0000 (44008.1860) grad_norm: 0.7532 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.74it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:55:50.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285880 data_time: 0.2475 (0.2488) loss: 1.2444 (2.2343) loss_scale: 32768.0000 (43752.7273) grad_norm: 1.4408 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:55:58.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mSave checkpoint 285900...\u001b[0m\n",
            "\u001b[32m2023-07-08 08:55:58.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1meval_step: n_samples=50, sample_steps=36mini_batch_size=8\u001b[0m\n",
            "\u001b[32m2023-07-08 08:55:58.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mPath for Eval images: /root/exp/default/0708_083413/eval_samples/285900_0708_085558\u001b[0m\n",
            "\n",
            "sample2dir:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "sample2dir:  14% 1/7 [00:14<01:26, 14.49s/it]\u001b[A\n",
            "sample2dir:  29% 2/7 [00:28<01:12, 14.43s/it]\u001b[A\n",
            "sample2dir:  43% 3/7 [00:43<00:57, 14.41s/it]\u001b[A\n",
            "sample2dir:  57% 4/7 [00:56<00:41, 13.76s/it]\u001b[A\n",
            "sample2dir:  71% 5/7 [01:10<00:27, 13.99s/it]\u001b[A\n",
            "sample2dir:  86% 6/7 [01:24<00:14, 14.15s/it]\u001b[A\n",
            "sample2dir: 100% 7/7 [01:39<00:00, 14.16s/it]\n",
            "\u001b[32m2023-07-08 08:57:37.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285900 data_time: 0.2427 (0.2486) loss: 1.3018 (2.2150) loss_scale: 32768.0000 (43508.6222) grad_norm: 1.0132 (inf)\u001b[0m\n",
            "\u001b[32m2023-07-08 08:57:37.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSave a grid of images...\u001b[0m\n",
            "Eval prompt: A chihuahua in watercolor painting style\n",
            "Shape of contexts :[torch.Size([10, 77, 1280])]\n",
            "epoch: 100% 3/3 [01:56<00:00, 38.85s/it]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.75it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.79it/s]\u001b[32m2023-07-08 08:58:00.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285920 data_time: 0.2421 (0.2485) loss: 1.1212 (2.1947) loss_scale: 32768.0000 (43275.1304) grad_norm: 0.3402 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.73it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.73it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:58:08.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285940 data_time: 0.2441 (0.2484) loss: 1.2975 (2.1767) loss_scale: 32768.0000 (43051.5745) grad_norm: 0.9604 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch:  67% 2/3 [00:00<00:00,  2.80it/s]\u001b[32m2023-07-08 08:58:15.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285960 data_time: 0.2426 (0.2483) loss: 1.1493 (2.1575) loss_scale: 32768.0000 (42837.3333) grad_norm: 0.6504 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch:  33% 1/3 [00:00<00:00,  2.81it/s]\u001b[32m2023-07-08 08:58:22.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 285980 data_time: 0.2463 (0.2483) loss: 1.1174 (2.1383) loss_scale: 32768.0000 (42631.8367) grad_norm: 0.3015 (inf)\u001b[0m\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "epoch: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "epoch:   0% 0/3 [00:00<?, ?it/s]\u001b[32m2023-07-08 08:58:29.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mSave checkpoint 286000...\u001b[0m\n",
            "\u001b[32m2023-07-08 08:58:29.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1meval_step: n_samples=50, sample_steps=36mini_batch_size=8\u001b[0m\n",
            "\u001b[32m2023-07-08 08:58:29.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36meval_step\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1mPath for Eval images: /root/exp/default/0708_083413/eval_samples/286000_0708_085829\u001b[0m\n",
            "\n",
            "sample2dir:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "sample2dir:  14% 1/7 [00:14<01:26, 14.34s/it]\u001b[A\n",
            "sample2dir:  29% 2/7 [00:28<01:11, 14.39s/it]\u001b[A\n",
            "sample2dir:  43% 3/7 [00:41<00:54, 13.72s/it]\u001b[A\n",
            "sample2dir:  57% 4/7 [00:56<00:42, 14.03s/it]\u001b[A\n",
            "sample2dir:  71% 5/7 [01:10<00:28, 14.18s/it]\u001b[A\n",
            "sample2dir:  86% 6/7 [01:25<00:14, 14.25s/it]\u001b[A\n",
            "sample2dir: 100% 7/7 [01:39<00:00, 14.18s/it]\n",
            "\u001b[32m2023-07-08 09:00:08.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m287\u001b[0m - \u001b[1mstep: 286000 data_time: 0.2421 (0.2481) loss: 1.1241 (2.1222) loss_scale: 32768.0000 (42434.5600) grad_norm: 0.3495 (inf)\u001b[0m\n",
            "\u001b[32m2023-07-08 09:00:08.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSave a grid of images...\u001b[0m\n",
            "Eval prompt: A chihuahua in watercolor painting style\n",
            "Shape of contexts :[torch.Size([10, 77, 1280])]\n",
            "epoch:   0% 0/3 [01:55<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir StyleDrop-PyTorch/style_adapter\n",
        "!mv /root/exp/default/0708_083413/ckpts_II/286000.ckpt StyleDrop-PyTorch/style_adapter/286000.ckpt\n"
      ],
      "metadata": {
        "id": "hr1k63BAGuSF"
      },
      "id": "hr1k63BAGuSF",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd StyleDrop-PyTorch && python3 gradio_demo.py"
      ],
      "metadata": {
        "id": "dv3ULWt4wDqH",
        "outputId": "e49a015e-1ee0-4bfd-9673-e2d0b83d105b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dv3ULWt4wDqH",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-07-08 09:15:22.457\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_clip.transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m314\u001b[0m - \u001b[34m\u001b[1mxattn in transformer of CLIP is True\u001b[0m\n",
            "\u001b[32m2023-07-08 09:15:37.114\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_clip.transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m314\u001b[0m - \u001b[34m\u001b[1mxattn in transformer of CLIP is True\u001b[0m\n",
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "Strict load\n",
            "Restored from assets/vqgan_jax_strongaug.ckpt\n",
            "xformers available, will use xformers attention\n",
            "\u001b[32m2023-07-08 09:15:56.554\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mlibs.uvit_t2i_vq\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m200\u001b[0m - \u001b[34m\u001b[1mcodebook size in nnet: 1024\u001b[0m\n",
            "num vis tokens: 256\n",
            "\u001b[32m2023-07-08 09:16:03.038\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mlibs.uvit_t2i_vq\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m200\u001b[0m - \u001b[34m\u001b[1mcodebook size in nnet: 1024\u001b[0m\n",
            "num vis tokens: 256\n",
            "\u001b[32m2023-07-08 09:16:09.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36minitialize_train_state\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mnnet has 505693313 parameters\u001b[0m\n",
            "\u001b[32m2023-07-08 09:16:10.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mresume\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mresume from assets/ckpts/cc3m-285000.ckpt\u001b[0m\n",
            "\u001b[32m2023-07-08 09:16:10.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mload from assets/ckpts/cc3m-285000.ckpt\u001b[0m\n",
            "/content/StyleDrop-PyTorch/gradio_demo.py:197: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
            "  result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(columns=2, height='auto')\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://d75a8f44ab46171d8e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Your style\n",
            "load adapter path: style_adapter/style.pth\n",
            "load adapter Done!\n",
            "torch.Size([1, 77, 1280])\n",
            "lambdaA: 2, lambdaB: 5, sample_steps: 36\n",
            "seed: 1234\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "torch.Size([1, 3, 256, 256])\n",
            "Your style\n",
            "load adapter path: style_adapter/style.pth\n",
            "load adapter Done!\n",
            "torch.Size([1, 77, 1280])\n",
            "lambdaA: 2, lambdaB: 5, sample_steps: 36\n",
            "seed: 1234\n",
            "torch.Size([1, 3, 256, 256])\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2125, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/StyleDrop-PyTorch/gradio_demo.py\", line 243, in <module>\n",
            "    block.launch(share=True,show_error=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2041, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2127, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d75a8f44ab46171d8e.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv StyleDrop-PyTorch/style_adapter/286000.ckpt/adapter.pth StyleDrop-PyTorch/style_adapter/style.pth"
      ],
      "metadata": {
        "id": "oGMj3yjTwK-U"
      },
      "id": "oGMj3yjTwK-U",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4w8j_ZxyIjk"
      },
      "id": "O4w8j_ZxyIjk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}